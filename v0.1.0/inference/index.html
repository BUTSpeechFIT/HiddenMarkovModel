<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Inference · MarkovModels</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MarkovModels</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../fsm/">Finite State Machines</a></li><li class="is-active"><a class="tocitem" href>Inference</a><ul class="internal"><li><a class="tocitem" href="#Baum-Welch-algoritm-(forward-backward)"><span>Baum-Welch algoritm (forward-backward)</span></a></li><li><a class="tocitem" href="#Getting-the-label-sequences"><span>Getting the label sequences</span></a></li><li><a class="tocitem" href="#Pruning"><span>Pruning</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Inference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Inference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/master/docs/src/inference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h1><h2 id="Baum-Welch-algoritm-(forward-backward)"><a class="docs-heading-anchor" href="#Baum-Welch-algoritm-(forward-backward)">Baum-Welch algoritm (forward-backward)</a><a id="Baum-Welch-algoritm-(forward-backward)-1"></a><a class="docs-heading-anchor-permalink" href="#Baum-Welch-algoritm-(forward-backward)" title="Permalink"></a></h2><p>The Baum-Welch algorithm computes the probability to be in a state <code>i</code> at time <span>$n$</span>:</p><p class="math-container">\[p(z_n = i | x_1, ..., x_N)\]</p><p>It is implemented by the <a href="#MarkovModels.αβrecursion"><code>αβrecursion</code></a> and the <a href="#MarkovModels.resps"><code>resps</code></a> functions.</p><article class="docstring"><header><a class="docstring-binding" id="MarkovModels.αβrecursion" href="#MarkovModels.αβrecursion"><code>MarkovModels.αβrecursion</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">αβrecursion(fsm, llh[, pruning = nopruning])</code></pre><p>Baum-Welch algorithm per state in the log domain. This function returns a tuple <code>(lnαβ, ttl)</code> where <code>lnαβ</code> is a sparse representation of the responsibilities and <code>ttl</code> is the log-likelihood of the sequence given the inference <code>fsm</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/3d457c4a8b17f32adda812ba14a350eeb23d9085/src/inference.jl#L66-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MarkovModels.resps" href="#MarkovModels.resps"><code>MarkovModels.resps</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">resps(fsm, lnαβ[, dense = false])</code></pre><p>Convert the output of the <code>αβrecursion</code> to the per-frame pdf responsibilities. The returned value is a dictionary whose keys are pdf indices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/3d457c4a8b17f32adda812ba14a350eeb23d9085/src/inference.jl#L108-L114">source</a></section></article><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>First, we create the inference FSM:</p><pre><code class="language-julia">using MarkovModels

emissionsmap = Dict(
    &quot;a&quot; =&gt; 1,
    &quot;b&quot; =&gt; 2,
    &quot;c&quot; =&gt; 1
)

fsm = LinearFSM([&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], emissionsmap)
for state in states(fsm)
    (isinit(state) || isfinal(state)) &amp;&amp; continue
    link!(state, state)
end
fsm = fsm |&gt; weightnormalize</code></pre><p><img src="../images/inference_fsm.svg" alt/></p><p>Note that state &quot;a&quot; and &quot;c&quot; share the same emission pdf.</p><p>As we don&#39;t have real distributions/data we simply simulate some fake per-pdf and per-frame log-likelihood:</p><pre><code class="language-julia">D, N = 2, 5 # number of pdfs, number of frames
llh = randn(D, N)</code></pre><p>Finally, we run the Baum-Welch algorithm:</p><pre><code class="language-julia">lnαβ, ttl = αβrecursion(fsm, llh)
γ = resps(fsm, lnαβ)

using Plots
p = plot()
plot!(p, γ[1], label = &quot;p(z = 1|X)&quot;)
plot!(p, γ[2], label = &quot;p(z = 2|X)&quot;)
plot!(p, γ[3], label = &quot;p(z = 3|X)&quot;)</code></pre><p><img src="../images/forward_backward_result.svg" alt/></p><h2 id="Getting-the-label-sequences"><a class="docs-heading-anchor" href="#Getting-the-label-sequences">Getting the label sequences</a><a id="Getting-the-label-sequences-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-the-label-sequences" title="Permalink"></a></h2><p>To generate a label sequence from some data, you can either:</p><ul><li>compute the most likely sequence of labels (see <a href="#MarkovModels.beststring"><code>beststring</code></a>)</li><li>draw a random sequence of label from <span>$p(W|x_1, ..., x_N)$</span> where <span>$W$</span> is a sequence of labels of any length (see <a href="#MarkovModels.samplestring"><code>samplestring</code></a>)</li></ul><article class="docstring"><header><a class="docstring-binding" id="MarkovModels.beststring" href="#MarkovModels.beststring"><code>MarkovModels.beststring</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">beststring(fsm, llh[, pruning = nopruning, labelfilter = x -&gt; true])</code></pre><p>Returns the best sequence of the labels given the log-likelihood <code>llh</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/3d457c4a8b17f32adda812ba14a350eeb23d9085/src/inference.jl#L175-L179">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MarkovModels.samplestring" href="#MarkovModels.samplestring"><code>MarkovModels.samplestring</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">samplestring(fsm, llh[, nsamples = 1, pruning = nopruning, labelfilter = x -&gt; true])</code></pre><p>Sample a sequence of labels given the log-likelihood <code>llh</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/3d457c4a8b17f32adda812ba14a350eeb23d9085/src/inference.jl#L186-L191">source</a></section></article><h2 id="Pruning"><a class="docs-heading-anchor" href="#Pruning">Pruning</a><a id="Pruning-1"></a><a class="docs-heading-anchor-permalink" href="#Pruning" title="Permalink"></a></h2><p>The inference functions (<a href="#MarkovModels.αβrecursion"><code>αβrecursion</code></a>, <a href="#MarkovModels.beststring"><code>beststring</code></a> and <a href="#MarkovModels.samplestring"><code>samplestring</code></a>) can be performed with a <em>pruning strategy</em>. This is necessary when the inference FSM is huge potentially leading to very long computational time. We propose two default strategies:</p><article class="docstring"><header><a class="docstring-binding" id="MarkovModels.SafePruning" href="#MarkovModels.SafePruning"><code>MarkovModels.SafePruning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct ThresholdPruning
    ...
end</code></pre><p>Prune the states that cannot reach the final state before the end of the sequence.</p><p><strong>Constructor</strong></p><pre><code class="language-julia">SafePruning(fsm)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/3d457c4a8b17f32adda812ba14a350eeb23d9085/src/pruning.jl#L18-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MarkovModels.ThresholdPruning" href="#MarkovModels.ThresholdPruning"><code>MarkovModels.ThresholdPruning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct ThresholdPruning
    ...
end</code></pre><p>Prune the active states (tokens) that have weights lower than the maximum weight at a given time frame minus a threshold <code>Δ</code>. The lower the threshold <code>Δ</code> the more the pruning:</p><ul><li>when <code>Δ &lt; 0</code> all paths are discarded, don&#39;t use negative threshold !!</li><li><code>Δ == 0</code> greedy decoding, only keep the most likely state at each time step</li><li><code>Δ = +∞</code> no pruning</li></ul><p><strong>Constructor</strong></p><pre><code class="language-julia">ThresholdPruning(Δ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/BUTSpeechFIT/MarkovModels/blob/3d457c4a8b17f32adda812ba14a350eeb23d9085/src/pruning.jl#L68-L85">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><a href="#MarkovModels.ThresholdPruning"><code>ThresholdPruning</code></a> is not <em>safe</em> in the sense that it does not guarantee that, after pruning, a valid path will remain. You can build a safe threshold-based pruning by combining (in the given order) the two strategies:</p><pre><code class="language-julia">ThresholdPruning(100) ∘ SafePruning(fsm)</code></pre></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../fsm/">« Finite State Machines</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 28 January 2021 16:26">Thursday 28 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
